# -*- coding: utf-8 -*-
"""Olx_Data_Science_Trainee_updated_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XPS1TB8hHkZSr8-7ahIpwishGKCcK91f

# **Import Necessary Packages**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import sklearn
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

df = pd.read_csv('otomoto_price_prediction_data.csv')  #Importing the CSV file.

df

"""# **Basic Pre-Processing**"""

df['fuel_type'].unique()  #Check for various fuel types.

#Check for various gearbox types and replace them with machine readable form.

df['gearbox'] = df['gearbox'].replace(['automatic','manual','cvt','dual-clutch','automatic-stepless','semi-automatic','automated-manual','automatic-sequential','automatic-stepless-sequential','manual-sequential'],[0,1,2,3,4,5,6,7,8,9]) # Label encode the gearbox column. 0: Automatic, 1: Manual

df['fuel_type'] = df['fuel_type'].replace(['petrol','diesel','petrol-lpg','hybrid','petrol-cng','electric','etanol'],[0,1,2,3,4,5,6])

df

df.isna().sum()  #Total number of missing values across each column.

percent_missing = df.isnull().sum() * 100 / len(df) #Total percentage of missing values.
percent_missing  #Since very less amount of data is missing across the whole dataframe, we can afford to drop them.

sns.heatmap(df.isnull(), cbar=False)

#This graphs helps us to visualize the locations of the various missing va values across all the columns. As we can probably see mileage has all the missing values in the middle of the column whereas engine_capacity,engine_power have na values spread across the respective columns.

#Alternate method.

#Fill the na values

df_mean = df['mileage'].fillna(df['mileage'].mean()) #filling in the mileage with average values
df_engine_capacity = df['engine_capacity'].fillna(df['engine_capacity'].mean()) #filling in the engine capacity with average values
df_engine_power = df['engine_power'].fillna(df['engine_power'].mean()) #filling in the engine power with average values
df_gearbox = df['gearbox'].fillna(df['gearbox'].mode()) #filling in the gearbox with most occuring value

df2_na_removed = df.dropna(axis='rows') #Dropping the na values altogether

df2_na_removed.isna().sum()

sns.heatmap(df2_na_removed.isnull(), cbar=False)

#No missing values

df2_na_removed

df2_na_removed['gearbox'].dtype

df2_na_removed['gearbox'] = df2_na_removed['gearbox'].astype(np.int64)  #Convert Gearbox column into int64 datatype.

df2_na_removed['gearbox'].dtype

df2_na_removed['gearbox'].unique()

make = df2_na_removed['make'].nunique()
print(make)

model = df2_na_removed['model'].nunique()
print(model)

#Due to high number of unrelated distinct values, we cannot take into consideration the first 2 values while performing a modelling procedure, although it should play a big part and as such create a tier of a group of cars with high to low brand value, same with model too

"""# **Exploratory Data Analysis**"""

df2_na_removed['damaged'].value_counts().plot(kind='bar',color ='grey')  #1: Vehicle is damaged ; 2: Vehicle is not damaged
plt.xlabel("Damaged", labelpad=14)

df2_na_removed['is_business'].value_counts().plot(kind='bar',color ='green')  #1:Business; 2: Individual
plt.xlabel("is_business", labelpad=14)

#Highly imbalanced, if a few models would have been there, we could have label encoded this column.


df2_na_removed['model'].value_counts().plot(kind='bar',color ='red') 
plt.xlabel("model")

#Highly imbalanced, if a few manufacturers would have been there, we could have label encoded this column.

df2_na_removed['make'].value_counts().plot(kind='barh')  
plt.xlabel("make", labelpad=14)

#f, ax = plt.subplots(figsize = (8,4))
sns.relplot(x = 'engine_capacity', y = 'target_price', data = df2_na_removed,style = df2_na_removed['fuel_type'])

#As we can see from here the target price is high when it comes for cars with fuel type as Petrol. We will dwelve further into this.

pd.value_counts(df2_na_removed['fuel_type']).plot(kind ='pie',colormap='CMRmap')
plt.title('Fuel Type', fontsize=10)

#Our hypothesis was right indeed. The fuel type Petrol has the highest number of row entries followed by diesel and very low percentage comprised of all other columns.

pd.value_counts(df2_na_removed['gearbox']).plot(kind ='pie',colormap='Spectral')

#It is evident from here that Manual transmission vehicles are much more prevalent with count above 120000 and automatic coming second with just above 60000 vechicles.

df2_na_removed.corr(method ='pearson')

f = plt.figure(figsize=(10, 10))
plt.matshow(df2_na_removed.corr(method='pearson'), fignum=f.number)
plt.xticks(range(df2_na_removed.select_dtypes(['number']).shape[1]), df2_na_removed.select_dtypes(['number']).columns, fontsize=8, rotation=45)
plt.yticks(range(df2_na_removed.select_dtypes(['number']).shape[1]), df2_na_removed.select_dtypes(['number']).columns, fontsize=8)
cb = plt.colorbar()
cb.ax.tick_params(labelsize=10)
#plt.title('Correlation Matrix', fontsize=10)


#Although there is not that high a correlation but as can be seen Engine power is correlated engine capacity with a value of 0.8

sns.relplot(x = 'vehicle_year', y = 'target_price',kind = 'line',hue = 'fuel_type', data = df2_na_removed)

#As deduced, we can see that the car price for fuel type as petrol rose to the maximum during the year 1960 whereas the newer fuel types have the maximum value as around 200000 to 250000 during the year 2002 to 2020.But still the older fuel types were much more in demand.

sns.catplot(y="gearbox", hue="fuel_type", kind="count",
            palette="pastel", edgecolor=".6",
            data=df2_na_removed)

print(df2_na_removed['vehicle_year'].min())

print(df2_na_removed['vehicle_year'].max())

sns.catplot(x='vehicle_year',
            y ='target_price',
                hue='fuel_type',
                data=df2_na_removed,
                kind='bar',
                ci=None,
                   )

sns.catplot(x='vehicle_year',
            y ='target_price',
                hue='gearbox',
                data=df2_na_removed,
                kind='bar',
                ci=None,
                   )

fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(df2_na_removed['engine_capacity'], df2_na_removed['target_price'], c = "black")
ax.set_xlabel('engine_capacity')
ax.set_ylabel('target_price')
plt.show()

fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(df2_na_removed['mileage'], df2_na_removed['target_price'],c='brown')
ax.set_xlabel('mileage')
ax.set_ylabel('target_price')
plt.show()

fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(df2_na_removed['engine_power'], df2_na_removed['target_price'], c = 'cyan')
ax.set_xlabel('mileage')
ax.set_ylabel('target_price')
plt.show()

"""# **Detection and Treating Outliers**"""

import seaborn as sns
sns.boxplot(df2_na_removed['mileage'])

sns.boxplot(df2_na_removed['engine_power'])

sns.boxplot(df2_na_removed['engine_capacity'])

q_low_mil = df2_na_removed["mileage"].quantile(0.01)
q_hi_mil = df2_na_removed["mileage"].quantile(0.99)

df2_na_removed= df2_na_removed[(df2_na_removed["mileage"] < q_hi_mil) & (df2_na_removed["mileage"] > q_low_mil)]

#print(df2_na_removed['mileage'].quantile(0.50)) 
#print(df2_na_removed['mileage'].quantile(0.95))

#df2_na_removed['mileage'] = np.where(df2_na_removed['mileage'] > 287000, 137800, df2_na_removed['mileage'])

sns.boxplot(df2_na_removed['mileage'])

#Since Engine power and Engine capacity are correlated to each other with a value of 0.8, It makes sense to remove one of them, as such, we will remove engine capacity since it has high values.

df2_na_removed = df2_na_removed.drop(['engine_capacity'], axis=1)

#print(df2_na_removed['engine_power'].quantile(0.50)) 
#print(df2_na_removed['engine_power'].quantile(0.95))

#df2_na_removed['engine_power'] = np.where(df2_na_removed['engine_power'] > 300, 136, df2_na_removed['engine_power'])

q_low_po = df2_na_removed["engine_power"].quantile(0.15)
q_hi_po  = df2_na_removed["engine_power"].quantile(0.85)

df2_na_removed= df2_na_removed[(df2_na_removed["engine_power"] < q_hi_po) & (df2_na_removed["engine_power"] > q_low_po)]

sns.boxplot(df2_na_removed['engine_power'])

df2_na_removed

X =df2_na_removed.drop(['target_price','make','model'],axis = 1)
y =df2_na_removed[['target_price']]

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)

"""# **Feature Scaling**"""

from sklearn.preprocessing import StandardScaler
sc =  StandardScaler()
scaled_features_train = X_train.copy()
scaled_features_test = X_test.copy()

col_names = ['mileage', 'engine_power']
features_train = scaled_features_train[col_names]
features_test = scaled_features_test[col_names]
scaler = sc.fit(features_train.values)
features = scaler.transform(features_test.values)
scaler_train = scaler.transform(features_train.values)

scaled_features_train[col_names] = scaler_train
scaled_features_test[col_names] = features


sc2 = StandardScaler()
scaler_test= sc2.fit(y_train)
y_scaled_test = scaler_test.transform(y_test)
y_scaled_train=scaler_test.transform(y_train)

scaled_features_train

scaled_features_test

"""# **K-Fold Cross Validation**"""

from sklearn.svm import SVR
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

scores = []
best_svr = SVR(kernel='rbf')
cv = KFold(n_splits=10, random_state=42, shuffle=False)

best_svr.fit(scaled_features_train, y_scaled_train)
scores.append(best_svr.score(scaled_features_test, y_scaled_test))


#cross_val_score(best_svr, X, y, cv=10)

scores

"""# **Support Vector Regression**"""

import timeit

start_svr = timeit.default_timer()

from sklearn.svm import SVR

regressor = SVR(kernel='rbf',C = 1)
regressor.fit(scaled_features_train,y_scaled_train)

stop_svr = timeit.default_timer()

print('Time: ', stop_svr - start_svr)

y_pred =regressor.predict(scaled_features_test)

y_pred

y_predicted = sc2.inverse_transform(y_pred)

y_predicted

y_test

from sklearn.metrics import r2_score 
r2_score(y_test, y_predicted)

from sklearn import metrics

print('Mean Absolute Error:', metrics.mean_absolute_error(y_scaled_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_scaled_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_scaled_test, y_pred)))

x_ax = range(len(y_predicted))
plt.scatter(x_ax, y_test, s=5, color="blue", label="original")
plt.plot(x_ax, y_predicted, lw=0.8, color="red", label="predicted")
plt.legend()
plt.show()

"""# **Custom CNN Model**"""

scaled_features_train = scaled_features_train.values.reshape(scaled_features_train.shape[0], scaled_features_train.shape[1], 1)
print(scaled_features_train.shape)

scaled_features_test = scaled_features_test.values.reshape(scaled_features_test.shape[0], scaled_features_test.shape[1], 1)
print(scaled_features_test.shape)

import tensorflow as tf

model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv1D(32, 2, activation="relu", input_shape=(7, 1)))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(64, activation="relu"))
model.add(tf.keras.layers.Dense(1))
model.compile(loss="mse", optimizer="adam")

model.summary()

model.compile(optimizer = 'adam',loss = 'mean_squared_error')

import timeit
start_cnn = timeit.default_timer()

model.fit(scaled_features_train, y_scaled_train, batch_size=100,epochs=50, verbose=0)

stop_cnn = timeit.default_timer()

print('Time: ', stop_cnn - start_cnn)

y_cnn_norm = model.predict(scaled_features_test)

y_cnn_pred = sc2.inverse_transform(model.predict(scaled_features_test))

y_cnn_pred

y_test

from sklearn.metrics import r2_score 
r2_score(y_test, y_cnn_pred)

from sklearn import metrics

print('Mean Absolute Error:', metrics.mean_absolute_error(y_scaled_test, y_cnn_norm))
print('Mean Squared Error:', metrics.mean_squared_error(y_scaled_test, y_cnn_norm))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_scaled_test, y_cnn_norm)))

x_ax = range(len(y_cnn_pred))
plt.scatter(x_ax, y_test, s=5, color="blue", label="original")
plt.plot(x_ax, y_cnn_pred, lw=0.8, color="red", label="predicted")
plt.legend()
plt.show()

"""# **Polynomial Regression**"""

from sklearn.preprocessing import PolynomialFeatures

P_reg = PolynomialFeatures(degree = 4)

nsamples, nx, ny = scaled_features_train.shape
d2_train_dataset = scaled_features_train.reshape((nsamples,nx*ny))

nsamples, nx, ny = scaled_features_test.shape
d2_test_dataset = scaled_features_test.reshape((nsamples,nx*ny))

X_P = P_reg.fit(d2_train_dataset)
X_Poly_train = X_P.transform(d2_train_dataset)
X_Poly_test = X_P.transform(d2_test_dataset)

from sklearn.linear_model import LinearRegression

start_poly = timeit.default_timer()

L_reg2 = LinearRegression()
L_reg2.fit(X_Poly_train,y_scaled_train)

stop_poly = timeit.default_timer()

print('Time: ', stop_poly - start_poly)

poly_pred = L_reg2.predict(X_Poly_test)

poly_predicted = sc2.inverse_transform(poly_pred)

y_test

poly_predicted

r2_score(y_test, poly_predicted)

from sklearn import metrics

print('Mean Absolute Error:', metrics.mean_absolute_error(y_scaled_test, poly_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_scaled_test, poly_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_scaled_test, poly_pred)))

x_ax = range(len(poly_predicted))
plt.scatter(x_ax, y_test, s=5, color="blue", label="original")
plt.plot(x_ax, poly_predicted, lw=0.8, color="red", label="predicted")
plt.legend()
plt.show()

"""# **Random Forest Regression**"""

from sklearn.ensemble import RandomForestRegressor
regressor_random = RandomForestRegressor(n_estimators = 10,random_state = 0)

X_random_train = scaled_features_train.reshape(scaled_features_train.shape[0], -1)
X_random_test = scaled_features_test.reshape(scaled_features_test.shape[0], -1)

import timeit
start_random = timeit.default_timer()

regressor_random.fit(X_random_train,y_scaled_train)

stop_random = timeit.default_timer()

print('Time: ', stop_random - start_random)

y_random_pred = sc2.inverse_transform(regressor_random.predict(X_random_test))

y_predictionn = regressor_random.predict(X_random_test)

y_random_pred

y_test

from sklearn.metrics import r2_score
r2_score(y_test,y_random_pred)

x_ax = range(len(y_random_pred))
plt.scatter(x_ax, y_test, s=5, color="blue", label="original")
plt.plot(x_ax, y_random_pred, lw=0.8, color="red", label="predicted")
plt.legend()
plt.show()

from sklearn import metrics

print('Mean Absolute Error:', metrics.mean_absolute_error(y_scaled_test, y_predictionn))
print('Mean Squared Error:', metrics.mean_squared_error(y_scaled_test, y_predictionn))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_scaled_test, y_predictionn)))

"""# **XG Boost Regressor**"""

import xgboost as xg

xgb_r = xg.XGBRegressor(objective ='reg:linear',
                  n_estimators = 100, seed = 123)

import timeit
start_random = timeit.default_timer()


xgb_r.fit(scaled_features_train,y_scaled_train)

stop_random = timeit.default_timer()

print('Time: ', stop_random - start_random)

y_xg = xgb_r.predict(scaled_features_test)

pred = sc2.inverse_transform(xgb_r.predict(scaled_features_test))

y_test

from sklearn.metrics import r2_score
r2_score(y_test,pred)

x_ax = range(len(pred))
plt.scatter(x_ax, y_test, s=5, color="blue", label="original")
plt.plot(x_ax,pred, lw=0.8, color="red", label="predicted")
plt.legend()
plt.show()

from sklearn import metrics

print('Mean Absolute Error:', metrics.mean_absolute_error(y_scaled_test, y_xg))
print('Mean Squared Error:', metrics.mean_squared_error(y_scaled_test, y_xg))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_scaled_test, y_xg)))

"""# **Model Performance**"""

#Support Vector Regression:- 0.386

#Custom Convolutional Neural Network:-0.802

#Polynomial Regression:-0.866

#Random Forest Regression:-0.873

#XG Boost Regression:- 0.874